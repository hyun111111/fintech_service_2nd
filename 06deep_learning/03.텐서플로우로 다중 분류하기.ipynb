{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c727bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e687768",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/haram4th/ADsP/main/iris3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668abec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b94fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d61d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"species\", axis=1)\n",
    "y = data[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea5cda5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le =   LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6496a",
   "metadata": {},
   "source": [
    "넘파이 어레이 말고, 원핫 인코딩으로 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf9f204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "0           True            False           False\n",
       "1           True            False           False\n",
       "2           True            False           False\n",
       "3           True            False           False\n",
       "4           True            False           False\n",
       "..           ...              ...             ...\n",
       "145        False            False            True\n",
       "146        False            False            True\n",
       "147        False            False            True\n",
       "148        False            False            True\n",
       "149        False            False            True\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c0fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f01b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47ed788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75be9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "train_temp = mms.fit_transform(X_train)\n",
    "test_temp = mms.transform(X_test)\n",
    "mms_X_train = pd.DataFrame(train_temp, columns= X_train.columns, index= X_train.index)\n",
    "mms_X_test = pd.DataFrame(test_temp, columns= X_test.columns, index= X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54731278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "123      0.588235         0.25      0.678571     0.708333\n",
       "24       0.147059         0.60      0.142857     0.041667\n",
       "25       0.205882         0.40      0.089286     0.041667\n",
       "23       0.235294         0.55      0.107143     0.166667\n",
       "94       0.382353         0.25      0.553571     0.500000\n",
       "..            ...          ...           ...          ...\n",
       "71       0.529412         0.30      0.517857     0.500000\n",
       "106      0.176471         0.15      0.607143     0.666667\n",
       "14       0.441176         0.90      0.017857     0.041667\n",
       "92       0.441176         0.20      0.517857     0.458333\n",
       "102      0.823529         0.40      0.857143     0.833333\n",
       "\n",
       "[90 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mms_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81396c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.035714</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1.058824</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "73       0.529412         0.30      0.642857     0.458333\n",
       "18       0.411765         0.80      0.107143     0.083333\n",
       "118      1.000000         0.20      1.035714     0.916667\n",
       "78       0.500000         0.35      0.607143     0.583333\n",
       "76       0.735294         0.30      0.660714     0.541667\n",
       "31       0.323529         0.60      0.071429     0.125000\n",
       "64       0.382353         0.35      0.446429     0.500000\n",
       "141      0.764706         0.45      0.714286     0.916667\n",
       "68       0.558824         0.00      0.607143     0.583333\n",
       "82       0.441176         0.25      0.500000     0.458333\n",
       "110      0.647059         0.50      0.714286     0.791667\n",
       "12       0.147059         0.40      0.053571     0.000000\n",
       "36       0.352941         0.65      0.035714     0.041667\n",
       "9        0.176471         0.45      0.071429     0.000000\n",
       "19       0.235294         0.80      0.071429     0.083333\n",
       "56       0.588235         0.55      0.642857     0.625000\n",
       "104      0.647059         0.40      0.839286     0.875000\n",
       "69       0.382353         0.15      0.500000     0.416667\n",
       "55       0.411765         0.30      0.607143     0.500000\n",
       "132      0.617647         0.30      0.803571     0.875000\n",
       "29       0.117647         0.50      0.089286     0.041667\n",
       "127      0.529412         0.40      0.678571     0.708333\n",
       "26       0.205882         0.60      0.089286     0.125000\n",
       "128      0.617647         0.30      0.803571     0.833333\n",
       "131      1.058824         0.80      0.946429     0.791667\n",
       "145      0.705882         0.40      0.732143     0.916667\n",
       "108      0.705882         0.15      0.839286     0.708333\n",
       "143      0.735294         0.50      0.857143     0.916667\n",
       "45       0.147059         0.40      0.053571     0.083333\n",
       "30       0.147059         0.45      0.089286     0.041667\n",
       "22       0.088235         0.70     -0.017857     0.041667\n",
       "15       0.411765         1.10      0.071429     0.125000\n",
       "65       0.705882         0.45      0.589286     0.541667\n",
       "11       0.147059         0.60      0.089286     0.041667\n",
       "42       0.029412         0.50      0.035714     0.041667\n",
       "146      0.588235         0.15      0.696429     0.750000\n",
       "51       0.617647         0.50      0.607143     0.583333\n",
       "27       0.264706         0.65      0.071429     0.041667\n",
       "4        0.205882         0.70      0.053571     0.041667\n",
       "32       0.264706         0.95      0.071429     0.000000\n",
       "142      0.441176         0.25      0.714286     0.750000\n",
       "85       0.500000         0.60      0.607143     0.625000\n",
       "86       0.705882         0.45      0.642857     0.583333\n",
       "16       0.323529         0.85      0.035714     0.125000\n",
       "10       0.323529         0.75      0.071429     0.041667\n",
       "81       0.352941         0.10      0.464286     0.375000\n",
       "133      0.588235         0.30      0.714286     0.583333\n",
       "137      0.617647         0.45      0.785714     0.708333\n",
       "75       0.676471         0.40      0.589286     0.541667\n",
       "109      0.852941         0.70      0.892857     1.000000\n",
       "96       0.411765         0.35      0.553571     0.500000\n",
       "105      0.970588         0.40      0.982143     0.833333\n",
       "66       0.382353         0.40      0.607143     0.583333\n",
       "0        0.235294         0.65      0.053571     0.041667\n",
       "122      1.000000         0.30      1.000000     0.791667\n",
       "67       0.441176         0.25      0.535714     0.375000\n",
       "28       0.264706         0.60      0.053571     0.041667\n",
       "40       0.205882         0.65      0.035714     0.083333\n",
       "44       0.235294         0.80      0.142857     0.125000\n",
       "60       0.205882        -0.10      0.428571     0.375000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mms_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c636462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "73         False             True           False\n",
       "18          True            False           False\n",
       "118        False            False            True\n",
       "78         False             True           False\n",
       "76         False             True           False\n",
       "31          True            False           False\n",
       "64         False             True           False\n",
       "141        False            False            True\n",
       "68         False             True           False\n",
       "82         False             True           False\n",
       "110        False            False            True\n",
       "12          True            False           False\n",
       "36          True            False           False\n",
       "9           True            False           False\n",
       "19          True            False           False\n",
       "56         False             True           False\n",
       "104        False            False            True\n",
       "69         False             True           False\n",
       "55         False             True           False\n",
       "132        False            False            True\n",
       "29          True            False           False\n",
       "127        False            False            True\n",
       "26          True            False           False\n",
       "128        False            False            True\n",
       "131        False            False            True\n",
       "145        False            False            True\n",
       "108        False            False            True\n",
       "143        False            False            True\n",
       "45          True            False           False\n",
       "30          True            False           False\n",
       "22          True            False           False\n",
       "15          True            False           False\n",
       "65         False             True           False\n",
       "11          True            False           False\n",
       "42          True            False           False\n",
       "146        False            False            True\n",
       "51         False             True           False\n",
       "27          True            False           False\n",
       "4           True            False           False\n",
       "32          True            False           False\n",
       "142        False            False            True\n",
       "85         False             True           False\n",
       "86         False             True           False\n",
       "16          True            False           False\n",
       "10          True            False           False\n",
       "81         False             True           False\n",
       "133        False            False            True\n",
       "137        False            False            True\n",
       "75         False             True           False\n",
       "109        False            False            True\n",
       "96         False             True           False\n",
       "105        False            False            True\n",
       "66         False             True           False\n",
       "0           True            False           False\n",
       "122        False            False            True\n",
       "67         False             True           False\n",
       "28          True            False           False\n",
       "40          True            False           False\n",
       "44          True            False           False\n",
       "60         False             True           False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ffe3d",
   "metadata": {},
   "source": [
    "# 텐서플로우로 다중분류 분석하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf3835",
   "metadata": {},
   "source": [
    "Sequencial API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87b7a8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 12:24:00.107976: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 12:24:00.118202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747279440.131448   55343 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747279440.135213   55343 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747279440.145517   55343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747279440.145541   55343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747279440.145542   55343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747279440.145543   55343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 12:24:00.149302: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96e1d063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mms_X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ad0d8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747279293.468800   47109 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1347 MB memory:  -> device: 0, name: NVIDIA GeForce MX450, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(mms_X_train.shape[1], )))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a073ab6",
   "metadata": {},
   "source": [
    "선생님 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df40d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747279447.867068   55343 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1347 MB memory:  -> device: 0, name: NVIDIA GeForce MX450, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(mms_X_train.shape[1], )))\n",
    "model.add(Dense(16,activation='relu')) # 데이터의 양이 적으므로 Dense를 적게 \n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf322b7",
   "metadata": {},
   "source": [
    "* 다중분류의 경우 사용하는 loss\n",
    "   * categorical_crossentropy: 종속변수 클래스가 3개 이상이고 on-hot encoding된 경우 \n",
    "   * sparse_categorical_crossentropy: 종속변수 클래스가 3개 이상인 경우이고 label encording된 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bd5ab63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m36\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m15\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267</span> (1.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m267\u001b[0m (1.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267</span> (1.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m267\u001b[0m (1.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6edef249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mms_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c1e1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747279455.431512   55393 service.cc:152] XLA service 0x7f4d00005cd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1747279455.431551   55393 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce MX450, Compute Capability 7.5\n",
      "2025-05-15 12:24:15.451119: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1747279455.610979   55393 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 1.0993"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747279456.793575   55393 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 339ms/step - accuracy: 0.3409 - loss: 1.1001 - val_accuracy: 0.3833 - val_loss: 1.0971\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4362 - loss: 1.0972 - val_accuracy: 0.6500 - val_loss: 1.0945\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6669 - loss: 1.0950 - val_accuracy: 0.6833 - val_loss: 1.0913\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6823 - loss: 1.0927 - val_accuracy: 0.6833 - val_loss: 1.0878\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6793 - loss: 1.0886 - val_accuracy: 0.6833 - val_loss: 1.0839\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6087 - loss: 1.0873 - val_accuracy: 0.6833 - val_loss: 1.0799\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6611 - loss: 1.0837 - val_accuracy: 0.6833 - val_loss: 1.0748\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6739 - loss: 1.0796 - val_accuracy: 0.6833 - val_loss: 1.0657\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6357 - loss: 1.0735 - val_accuracy: 0.6833 - val_loss: 1.0530\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6636 - loss: 1.0603 - val_accuracy: 0.6833 - val_loss: 1.0390\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7057 - loss: 1.0454 - val_accuracy: 0.7333 - val_loss: 1.0245\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7187 - loss: 1.0464 - val_accuracy: 0.7833 - val_loss: 1.0109\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7595 - loss: 1.0321 - val_accuracy: 0.7500 - val_loss: 0.9965\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6735 - loss: 1.0227 - val_accuracy: 0.7167 - val_loss: 0.9810\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6366 - loss: 1.0178 - val_accuracy: 0.6833 - val_loss: 0.9649\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6555 - loss: 0.9835 - val_accuracy: 0.6833 - val_loss: 0.9475\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6825 - loss: 0.9763 - val_accuracy: 0.6833 - val_loss: 0.9297\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6651 - loss: 0.9545 - val_accuracy: 0.6833 - val_loss: 0.9112\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6778 - loss: 0.9445 - val_accuracy: 0.6833 - val_loss: 0.8926\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6136 - loss: 0.9450 - val_accuracy: 0.6833 - val_loss: 0.8740\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6861 - loss: 0.9046 - val_accuracy: 0.6833 - val_loss: 0.8544\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6325 - loss: 0.9132 - val_accuracy: 0.6833 - val_loss: 0.8352\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6715 - loss: 0.8971 - val_accuracy: 0.6833 - val_loss: 0.8163\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7126 - loss: 0.8452 - val_accuracy: 0.6833 - val_loss: 0.7980\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7081 - loss: 0.8464 - val_accuracy: 0.6833 - val_loss: 0.7807\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6538 - loss: 0.8824 - val_accuracy: 0.6833 - val_loss: 0.7653\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6278 - loss: 0.8615 - val_accuracy: 0.6833 - val_loss: 0.7507\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6093 - loss: 0.8477 - val_accuracy: 0.6833 - val_loss: 0.7369\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6450 - loss: 0.8104 - val_accuracy: 0.6833 - val_loss: 0.7244\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6525 - loss: 0.8238 - val_accuracy: 0.6833 - val_loss: 0.7135\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6921 - loss: 0.7767 - val_accuracy: 0.6833 - val_loss: 0.7034\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6653 - loss: 0.7996 - val_accuracy: 0.6833 - val_loss: 0.6948\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6184 - loss: 0.7825 - val_accuracy: 0.6833 - val_loss: 0.6862\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6845 - loss: 0.7546 - val_accuracy: 0.6833 - val_loss: 0.6788\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7288 - loss: 0.7361 - val_accuracy: 0.6833 - val_loss: 0.6721\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5964 - loss: 0.7636 - val_accuracy: 0.6833 - val_loss: 0.6657\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6613 - loss: 0.7453 - val_accuracy: 0.6833 - val_loss: 0.6603\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6616 - loss: 0.7364 - val_accuracy: 0.6833 - val_loss: 0.6555\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6683 - loss: 0.7198 - val_accuracy: 0.6833 - val_loss: 0.6511\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6816 - loss: 0.6714 - val_accuracy: 0.6833 - val_loss: 0.6470\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6733 - loss: 0.7413 - val_accuracy: 0.6833 - val_loss: 0.6434\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5891 - loss: 0.7634 - val_accuracy: 0.6833 - val_loss: 0.6399\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6489 - loss: 0.6926 - val_accuracy: 0.6833 - val_loss: 0.6365\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6020 - loss: 0.7766 - val_accuracy: 0.6833 - val_loss: 0.6334\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6703 - loss: 0.7274 - val_accuracy: 0.6833 - val_loss: 0.6304\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6458 - loss: 0.7030 - val_accuracy: 0.6833 - val_loss: 0.6215\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6153 - loss: 0.7312 - val_accuracy: 0.6833 - val_loss: 0.6060\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6693 - loss: 0.6691 - val_accuracy: 0.6833 - val_loss: 0.5927\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6730 - loss: 0.6772 - val_accuracy: 0.7167 - val_loss: 0.5803\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6827 - loss: 0.6319 - val_accuracy: 0.6833 - val_loss: 0.5704\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6750 - loss: 0.7059 - val_accuracy: 0.7000 - val_loss: 0.5577\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6377 - loss: 0.6542 - val_accuracy: 0.6833 - val_loss: 0.5464\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6489 - loss: 0.6181 - val_accuracy: 0.6833 - val_loss: 0.5361\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6397 - loss: 0.6538 - val_accuracy: 0.6833 - val_loss: 0.5262\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6453 - loss: 0.6086 - val_accuracy: 0.6833 - val_loss: 0.5174\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7302 - loss: 0.5368 - val_accuracy: 0.8333 - val_loss: 0.5082\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7362 - loss: 0.6304 - val_accuracy: 0.8333 - val_loss: 0.4992\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8022 - loss: 0.5566 - val_accuracy: 0.8333 - val_loss: 0.4910\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8342 - loss: 0.5627 - val_accuracy: 0.8333 - val_loss: 0.4827\n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8123 - loss: 0.5530 - val_accuracy: 0.8667 - val_loss: 0.4749\n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7910 - loss: 0.5840 - val_accuracy: 0.8667 - val_loss: 0.4673\n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8061 - loss: 0.5230 - val_accuracy: 0.8333 - val_loss: 0.4605\n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7825 - loss: 0.5545 - val_accuracy: 0.8667 - val_loss: 0.4529\n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8062 - loss: 0.5194 - val_accuracy: 0.8667 - val_loss: 0.4460\n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8578 - loss: 0.4759 - val_accuracy: 0.8500 - val_loss: 0.4399\n",
      "Epoch 66/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8290 - loss: 0.5237 - val_accuracy: 0.9000 - val_loss: 0.4334\n",
      "Epoch 67/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8381 - loss: 0.5081 - val_accuracy: 0.9000 - val_loss: 0.4274\n",
      "Epoch 68/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8365 - loss: 0.5107 - val_accuracy: 0.9000 - val_loss: 0.4204\n",
      "Epoch 69/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8696 - loss: 0.4726 - val_accuracy: 0.8833 - val_loss: 0.4149\n",
      "Epoch 70/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8066 - loss: 0.5075 - val_accuracy: 0.9333 - val_loss: 0.4088\n",
      "Epoch 71/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8990 - loss: 0.4493 - val_accuracy: 0.9333 - val_loss: 0.4033\n",
      "Epoch 72/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8649 - loss: 0.4490 - val_accuracy: 0.9333 - val_loss: 0.3985\n",
      "Epoch 73/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8704 - loss: 0.4898 - val_accuracy: 0.9167 - val_loss: 0.3924\n",
      "Epoch 74/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8616 - loss: 0.4391 - val_accuracy: 0.9333 - val_loss: 0.3875\n",
      "Epoch 75/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8649 - loss: 0.4371 - val_accuracy: 0.9333 - val_loss: 0.3824\n",
      "Epoch 76/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8705 - loss: 0.4447 - val_accuracy: 0.9333 - val_loss: 0.3774\n",
      "Epoch 77/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8783 - loss: 0.4058 - val_accuracy: 0.9333 - val_loss: 0.3725\n",
      "Epoch 78/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8768 - loss: 0.4568 - val_accuracy: 0.9333 - val_loss: 0.3682\n",
      "Epoch 79/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8979 - loss: 0.4283 - val_accuracy: 0.9500 - val_loss: 0.3644\n",
      "Epoch 80/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9108 - loss: 0.4332 - val_accuracy: 0.9500 - val_loss: 0.3591\n",
      "Epoch 81/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8679 - loss: 0.4672 - val_accuracy: 0.9500 - val_loss: 0.3547\n",
      "Epoch 82/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9034 - loss: 0.4149 - val_accuracy: 0.9500 - val_loss: 0.3507\n",
      "Epoch 83/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9217 - loss: 0.3761 - val_accuracy: 0.9500 - val_loss: 0.3467\n",
      "Epoch 84/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9192 - loss: 0.3750 - val_accuracy: 0.9667 - val_loss: 0.3433\n",
      "Epoch 85/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9361 - loss: 0.3671 - val_accuracy: 0.9667 - val_loss: 0.3412\n",
      "Epoch 86/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9296 - loss: 0.4233 - val_accuracy: 0.9667 - val_loss: 0.3373\n",
      "Epoch 87/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9127 - loss: 0.4095 - val_accuracy: 0.9500 - val_loss: 0.3315\n",
      "Epoch 88/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9120 - loss: 0.4215 - val_accuracy: 0.9500 - val_loss: 0.3281\n",
      "Epoch 89/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9149 - loss: 0.3967 - val_accuracy: 0.9500 - val_loss: 0.3248\n",
      "Epoch 90/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9197 - loss: 0.3891 - val_accuracy: 0.9667 - val_loss: 0.3216\n",
      "Epoch 91/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9145 - loss: 0.3609 - val_accuracy: 0.9667 - val_loss: 0.3186\n",
      "Epoch 92/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8994 - loss: 0.4226 - val_accuracy: 0.9667 - val_loss: 0.3172\n",
      "Epoch 93/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9328 - loss: 0.3774 - val_accuracy: 0.9667 - val_loss: 0.3126\n",
      "Epoch 94/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8986 - loss: 0.4125 - val_accuracy: 0.9667 - val_loss: 0.3100\n",
      "Epoch 95/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9180 - loss: 0.3958 - val_accuracy: 0.9667 - val_loss: 0.3066\n",
      "Epoch 96/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9071 - loss: 0.4016 - val_accuracy: 0.9667 - val_loss: 0.3036\n",
      "Epoch 97/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9354 - loss: 0.3868 - val_accuracy: 0.9667 - val_loss: 0.3012\n",
      "Epoch 98/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9529 - loss: 0.3387 - val_accuracy: 0.9667 - val_loss: 0.2999\n",
      "Epoch 99/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9281 - loss: 0.3488 - val_accuracy: 0.9667 - val_loss: 0.2972\n",
      "Epoch 100/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9296 - loss: 0.3656 - val_accuracy: 0.9667 - val_loss: 0.2938\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537ms/step\n"
     ]
    }
   ],
   "source": [
    "history =  model.fit(mms_X_train, y_train, epochs=100, batch_size=16,\n",
    "                    validation_data=(mms_X_test,y_test))\n",
    "pred =  model.predict(mms_X_test)\n",
    "pred = pd.DataFrame(pred, index=y_test.index)\n",
    "result = pd.concat([y_test, pred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e75575a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343ms/step - accuracy: 0.9569 - loss: 0.2909\n",
      "test loss : 0.29383549094200134\n",
      "testaccuracy : 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(mms_X_test, y_test)\n",
    "print('test loss :',score[0] )\n",
    "print('testaccuracy :',score[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e8ac1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "loss\n",
      "val_accuracy\n",
      "val_loss\n"
     ]
    }
   ],
   "source": [
    "for i in history.history:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b02d5bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85d36eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998851</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.019142</td>\n",
       "      <td>0.022188</td>\n",
       "      <td>0.958670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.185060</td>\n",
       "      <td>0.455492</td>\n",
       "      <td>0.359448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.173056</td>\n",
       "      <td>0.403826</td>\n",
       "      <td>0.423119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.996821</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.043306</td>\n",
       "      <td>0.061623</td>\n",
       "      <td>0.895071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.120961</td>\n",
       "      <td>0.235749</td>\n",
       "      <td>0.643290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.093287</td>\n",
       "      <td>0.165668</td>\n",
       "      <td>0.741045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.996280</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.001127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998826</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.997037</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.000890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999178</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.034911</td>\n",
       "      <td>0.937547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.030241</td>\n",
       "      <td>0.039240</td>\n",
       "      <td>0.930519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.997141</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.118041</td>\n",
       "      <td>0.227878</td>\n",
       "      <td>0.654080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.997106</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.000868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.032372</td>\n",
       "      <td>0.042734</td>\n",
       "      <td>0.924893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.088920</td>\n",
       "      <td>0.155465</td>\n",
       "      <td>0.755615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.035979</td>\n",
       "      <td>0.048792</td>\n",
       "      <td>0.915229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.042288</td>\n",
       "      <td>0.059798</td>\n",
       "      <td>0.897914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.024299</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>0.945841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.994941</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.996259</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.001133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999157</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998277</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.997334</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.057715</td>\n",
       "      <td>0.088748</td>\n",
       "      <td>0.853537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998695</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.000271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>0.082424</td>\n",
       "      <td>0.863109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999154</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.135153</td>\n",
       "      <td>0.275844</td>\n",
       "      <td>0.589003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.091108</td>\n",
       "      <td>0.160550</td>\n",
       "      <td>0.748342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.024014</td>\n",
       "      <td>0.029422</td>\n",
       "      <td>0.946564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.022069</td>\n",
       "      <td>0.026483</td>\n",
       "      <td>0.951448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>0.029440</td>\n",
       "      <td>0.946534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998389</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998592</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998455</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.322689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iris-setosa  Iris-versicolor  Iris-virginica         0         1  \\\n",
       "73         False             True           False  0.191183  0.486129   \n",
       "18          True            False           False  0.998851  0.000817   \n",
       "118        False            False            True  0.019142  0.022188   \n",
       "78         False             True           False  0.185060  0.455492   \n",
       "76         False             True           False  0.173056  0.403826   \n",
       "31          True            False           False  0.996821  0.002223   \n",
       "64         False             True           False  0.191183  0.486129   \n",
       "141        False            False            True  0.043306  0.061623   \n",
       "68         False             True           False  0.120961  0.235749   \n",
       "82         False             True           False  0.191183  0.486129   \n",
       "110        False            False            True  0.093287  0.165668   \n",
       "12          True            False           False  0.996280  0.002593   \n",
       "36          True            False           False  0.998826  0.000834   \n",
       "9           True            False           False  0.997037  0.002074   \n",
       "19          True            False           False  0.999178  0.000587   \n",
       "56         False             True           False  0.191183  0.486129   \n",
       "104        False            False            True  0.027542  0.034911   \n",
       "69         False             True           False  0.191183  0.486129   \n",
       "55         False             True           False  0.191183  0.486129   \n",
       "132        False            False            True  0.030241  0.039240   \n",
       "29          True            False           False  0.997141  0.002002   \n",
       "127        False            False            True  0.118041  0.227878   \n",
       "26          True            False           False  0.997106  0.002026   \n",
       "128        False            False            True  0.032372  0.042734   \n",
       "131        False            False            True  0.088920  0.155465   \n",
       "145        False            False            True  0.035979  0.048792   \n",
       "108        False            False            True  0.042288  0.059798   \n",
       "143        False            False            True  0.024299  0.029859   \n",
       "45          True            False           False  0.994941  0.003509   \n",
       "30          True            False           False  0.996259  0.002607   \n",
       "22          True            False           False  0.999157  0.000602   \n",
       "15          True            False           False  0.999711  0.000209   \n",
       "65         False             True           False  0.191183  0.486129   \n",
       "11          True            False           False  0.998277  0.001216   \n",
       "42          True            False           False  0.997334  0.001869   \n",
       "146        False            False            True  0.057715  0.088748   \n",
       "51         False             True           False  0.191183  0.486129   \n",
       "27          True            False           False  0.998695  0.000925   \n",
       "4           True            False           False  0.999055  0.000674   \n",
       "32          True            False           False  0.999729  0.000197   \n",
       "142        False            False            True  0.054467  0.082424   \n",
       "85         False             True           False  0.191183  0.486129   \n",
       "86         False             True           False  0.191183  0.486129   \n",
       "16          True            False           False  0.999275  0.000519   \n",
       "10          True            False           False  0.999154  0.000604   \n",
       "81         False             True           False  0.191183  0.486129   \n",
       "133        False            False            True  0.135153  0.275844   \n",
       "137        False            False            True  0.091108  0.160550   \n",
       "75         False             True           False  0.191183  0.486129   \n",
       "109        False            False            True  0.024014  0.029422   \n",
       "96         False             True           False  0.191183  0.486129   \n",
       "105        False            False            True  0.022069  0.026483   \n",
       "66         False             True           False  0.191183  0.486129   \n",
       "0           True            False           False  0.998769  0.000874   \n",
       "122        False            False            True  0.024025  0.029440   \n",
       "67         False             True           False  0.191183  0.486129   \n",
       "28          True            False           False  0.998389  0.001138   \n",
       "40          True            False           False  0.998592  0.000998   \n",
       "44          True            False           False  0.998455  0.001091   \n",
       "60         False             True           False  0.191183  0.486129   \n",
       "\n",
       "            2  \n",
       "73   0.322689  \n",
       "18   0.000333  \n",
       "118  0.958670  \n",
       "78   0.359448  \n",
       "76   0.423119  \n",
       "31   0.000956  \n",
       "64   0.322689  \n",
       "141  0.895071  \n",
       "68   0.643290  \n",
       "82   0.322689  \n",
       "110  0.741045  \n",
       "12   0.001127  \n",
       "36   0.000340  \n",
       "9    0.000890  \n",
       "19   0.000235  \n",
       "56   0.322689  \n",
       "104  0.937547  \n",
       "69   0.322689  \n",
       "55   0.322689  \n",
       "132  0.930519  \n",
       "29   0.000857  \n",
       "127  0.654080  \n",
       "26   0.000868  \n",
       "128  0.924893  \n",
       "131  0.755615  \n",
       "145  0.915229  \n",
       "108  0.897914  \n",
       "143  0.945841  \n",
       "45   0.001550  \n",
       "30   0.001133  \n",
       "22   0.000241  \n",
       "15   0.000079  \n",
       "65   0.322689  \n",
       "11   0.000506  \n",
       "42   0.000797  \n",
       "146  0.853537  \n",
       "51   0.322689  \n",
       "27   0.000379  \n",
       "4    0.000271  \n",
       "32   0.000074  \n",
       "142  0.863109  \n",
       "85   0.322689  \n",
       "86   0.322689  \n",
       "16   0.000206  \n",
       "10   0.000242  \n",
       "81   0.322689  \n",
       "133  0.589003  \n",
       "137  0.748342  \n",
       "75   0.322689  \n",
       "109  0.946564  \n",
       "96   0.322689  \n",
       "105  0.951448  \n",
       "66   0.322689  \n",
       "0    0.000357  \n",
       "122  0.946534  \n",
       "67   0.322689  \n",
       "28   0.000472  \n",
       "40   0.000410  \n",
       "44   0.000454  \n",
       "60   0.322689  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d351ab",
   "metadata": {},
   "source": [
    "Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "beaf29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16d91242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m36\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m15\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267</span> (1.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m267\u001b[0m (1.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267</span> (1.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m267\u001b[0m (1.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs =  Input(shape=(mms_X_train.shape[1],))\n",
    "\n",
    "x = Dense(16, activation='relu')(inputs)\n",
    "x = Dense(8, activation='relu')(x)\n",
    "x = Dense(4, activation='relu')(x)\n",
    "\n",
    "outputs = Dense(3, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71478c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85ec9183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 235ms/step - accuracy: 0.4395 - loss: 1.1023 - val_accuracy: 0.5500 - val_loss: 1.1072\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.3974 - loss: 1.1226 - val_accuracy: 0.3167 - val_loss: 1.1040\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4032 - loss: 1.0979 - val_accuracy: 0.3167 - val_loss: 1.1019\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3166 - loss: 1.1055 - val_accuracy: 0.3167 - val_loss: 1.0998\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3753 - loss: 1.0938 - val_accuracy: 0.3167 - val_loss: 1.0982\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.2928 - loss: 1.1043 - val_accuracy: 0.3167 - val_loss: 1.0969\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3100 - loss: 1.0967 - val_accuracy: 0.3167 - val_loss: 1.0957\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3163 - loss: 1.0994 - val_accuracy: 0.3333 - val_loss: 1.0946\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3861 - loss: 1.0911 - val_accuracy: 0.3667 - val_loss: 1.0934\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3007 - loss: 1.0994 - val_accuracy: 0.3667 - val_loss: 1.0927\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3986 - loss: 1.0912 - val_accuracy: 0.3667 - val_loss: 1.0920\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4039 - loss: 1.0880 - val_accuracy: 0.3667 - val_loss: 1.0912\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.3685 - loss: 1.0913 - val_accuracy: 0.3667 - val_loss: 1.0900\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3748 - loss: 1.0862 - val_accuracy: 0.3667 - val_loss: 1.0892\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.3700 - loss: 1.0899 - val_accuracy: 0.3667 - val_loss: 1.0885\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4722 - loss: 1.0838 - val_accuracy: 0.3833 - val_loss: 1.0878\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4576 - loss: 1.0835 - val_accuracy: 0.3667 - val_loss: 1.0868\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4603 - loss: 1.0802 - val_accuracy: 0.3667 - val_loss: 1.0856\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4202 - loss: 1.0765 - val_accuracy: 0.3667 - val_loss: 1.0846\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4307 - loss: 1.0787 - val_accuracy: 0.3667 - val_loss: 1.0834\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4308 - loss: 1.0782 - val_accuracy: 0.3833 - val_loss: 1.0823\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4670 - loss: 1.0680 - val_accuracy: 0.3833 - val_loss: 1.0810\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4690 - loss: 1.0721 - val_accuracy: 0.3833 - val_loss: 1.0798\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4467 - loss: 1.0663 - val_accuracy: 0.3667 - val_loss: 1.0780\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4335 - loss: 1.0754 - val_accuracy: 0.3667 - val_loss: 1.0765\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4593 - loss: 1.0650 - val_accuracy: 0.3667 - val_loss: 1.0743\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4205 - loss: 1.0599 - val_accuracy: 0.3667 - val_loss: 1.0726\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.3806 - loss: 1.0662 - val_accuracy: 0.3667 - val_loss: 1.0712\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4624 - loss: 1.0524 - val_accuracy: 0.3667 - val_loss: 1.0695\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4192 - loss: 1.0643 - val_accuracy: 0.4000 - val_loss: 1.0680\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4397 - loss: 1.0647 - val_accuracy: 0.3833 - val_loss: 1.0659\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4033 - loss: 1.0628 - val_accuracy: 0.3667 - val_loss: 1.0631\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3991 - loss: 1.0632 - val_accuracy: 0.3667 - val_loss: 1.0606\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3938 - loss: 1.0554 - val_accuracy: 0.3667 - val_loss: 1.0584\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3706 - loss: 1.0629 - val_accuracy: 0.3833 - val_loss: 1.0563\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4625 - loss: 1.0479 - val_accuracy: 0.4000 - val_loss: 1.0538\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4771 - loss: 1.0300 - val_accuracy: 0.3667 - val_loss: 1.0505\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4109 - loss: 1.0472 - val_accuracy: 0.3667 - val_loss: 1.0480\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4844 - loss: 1.0136 - val_accuracy: 0.3667 - val_loss: 1.0450\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5082 - loss: 1.0237 - val_accuracy: 0.4000 - val_loss: 1.0421\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4527 - loss: 1.0463 - val_accuracy: 0.4000 - val_loss: 1.0389\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5391 - loss: 1.0211 - val_accuracy: 0.3667 - val_loss: 1.0352\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4011 - loss: 1.0396 - val_accuracy: 0.3667 - val_loss: 1.0319\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4566 - loss: 1.0234 - val_accuracy: 0.3667 - val_loss: 1.0283\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4372 - loss: 1.0060 - val_accuracy: 0.3667 - val_loss: 1.0244\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4229 - loss: 1.0300 - val_accuracy: 0.4000 - val_loss: 1.0224\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4595 - loss: 1.0154 - val_accuracy: 0.4167 - val_loss: 1.0189\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4869 - loss: 0.9947 - val_accuracy: 0.3833 - val_loss: 1.0138\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4824 - loss: 1.0045 - val_accuracy: 0.3833 - val_loss: 1.0097\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3982 - loss: 1.0084 - val_accuracy: 0.4000 - val_loss: 1.0064\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4848 - loss: 0.9912 - val_accuracy: 0.4000 - val_loss: 1.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4464 - loss: 0.9974 - val_accuracy: 0.3833 - val_loss: 0.9979\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4360 - loss: 0.9867 - val_accuracy: 0.3833 - val_loss: 0.9935\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4470 - loss: 0.9974 - val_accuracy: 0.4000 - val_loss: 0.9896\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4829 - loss: 0.9888 - val_accuracy: 0.4000 - val_loss: 0.9857\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4828 - loss: 0.9683 - val_accuracy: 0.4000 - val_loss: 0.9810\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4375 - loss: 0.9793 - val_accuracy: 0.3833 - val_loss: 0.9757\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4725 - loss: 0.9586 - val_accuracy: 0.3833 - val_loss: 0.9711\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4662 - loss: 0.9480 - val_accuracy: 0.4000 - val_loss: 0.9665\n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4829 - loss: 0.9580 - val_accuracy: 0.4167 - val_loss: 0.9621\n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4742 - loss: 0.9455 - val_accuracy: 0.4000 - val_loss: 0.9565\n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4961 - loss: 0.9243 - val_accuracy: 0.3833 - val_loss: 0.9510\n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4966 - loss: 0.9248 - val_accuracy: 0.4000 - val_loss: 0.9458\n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4381 - loss: 0.9422 - val_accuracy: 0.4667 - val_loss: 0.9414\n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4885 - loss: 0.9324 - val_accuracy: 0.4667 - val_loss: 0.9358\n",
      "Epoch 66/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5298 - loss: 0.9161 - val_accuracy: 0.4500 - val_loss: 0.9301\n",
      "Epoch 67/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4511 - loss: 0.9508 - val_accuracy: 0.4667 - val_loss: 0.9254\n",
      "Epoch 68/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5306 - loss: 0.9075 - val_accuracy: 0.4333 - val_loss: 0.9194\n",
      "Epoch 69/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4754 - loss: 0.9274 - val_accuracy: 0.4500 - val_loss: 0.9145\n",
      "Epoch 70/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4801 - loss: 0.9328 - val_accuracy: 0.4667 - val_loss: 0.9093\n",
      "Epoch 71/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5082 - loss: 0.9071 - val_accuracy: 0.4833 - val_loss: 0.9044\n",
      "Epoch 72/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5088 - loss: 0.9001 - val_accuracy: 0.5000 - val_loss: 0.8994\n",
      "Epoch 73/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4950 - loss: 0.9163 - val_accuracy: 0.5000 - val_loss: 0.8960\n",
      "Epoch 74/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5528 - loss: 0.8343 - val_accuracy: 0.5000 - val_loss: 0.8880\n",
      "Epoch 75/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4484 - loss: 0.9335 - val_accuracy: 0.5000 - val_loss: 0.8843\n",
      "Epoch 76/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4411 - loss: 0.8988 - val_accuracy: 0.5167 - val_loss: 0.8800\n",
      "Epoch 77/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4783 - loss: 0.8923 - val_accuracy: 0.5167 - val_loss: 0.8727\n",
      "Epoch 78/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4913 - loss: 0.8819 - val_accuracy: 0.5167 - val_loss: 0.8646\n",
      "Epoch 79/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5486 - loss: 0.8314 - val_accuracy: 0.5000 - val_loss: 0.8549\n",
      "Epoch 80/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5094 - loss: 0.8809 - val_accuracy: 0.5333 - val_loss: 0.8437\n",
      "Epoch 81/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5437 - loss: 0.8282 - val_accuracy: 0.5333 - val_loss: 0.8285\n",
      "Epoch 82/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5552 - loss: 0.8221 - val_accuracy: 0.5833 - val_loss: 0.8112\n",
      "Epoch 83/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5589 - loss: 0.8182 - val_accuracy: 0.6000 - val_loss: 0.7909\n",
      "Epoch 84/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6282 - loss: 0.7606 - val_accuracy: 0.6000 - val_loss: 0.7659\n",
      "Epoch 85/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5730 - loss: 0.7524 - val_accuracy: 0.6000 - val_loss: 0.7412\n",
      "Epoch 86/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6458 - loss: 0.7157 - val_accuracy: 0.6000 - val_loss: 0.7229\n",
      "Epoch 87/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6397 - loss: 0.6831 - val_accuracy: 0.6000 - val_loss: 0.6978\n",
      "Epoch 88/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6277 - loss: 0.6960 - val_accuracy: 0.6000 - val_loss: 0.6814\n",
      "Epoch 89/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6212 - loss: 0.6690 - val_accuracy: 0.6000 - val_loss: 0.6642\n",
      "Epoch 90/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6770 - loss: 0.6128 - val_accuracy: 0.6000 - val_loss: 0.6468\n",
      "Epoch 91/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6356 - loss: 0.6136 - val_accuracy: 0.6000 - val_loss: 0.6268\n",
      "Epoch 92/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6015 - loss: 0.6282 - val_accuracy: 0.6000 - val_loss: 0.6128\n",
      "Epoch 93/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6387 - loss: 0.5923 - val_accuracy: 0.6000 - val_loss: 0.6009\n",
      "Epoch 94/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7020 - loss: 0.5305 - val_accuracy: 0.6000 - val_loss: 0.5885\n",
      "Epoch 95/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6536 - loss: 0.5584 - val_accuracy: 0.6000 - val_loss: 0.5789\n",
      "Epoch 96/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6087 - loss: 0.5722 - val_accuracy: 0.6000 - val_loss: 0.5680\n",
      "Epoch 97/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6386 - loss: 0.5371 - val_accuracy: 0.5833 - val_loss: 0.5618\n",
      "Epoch 98/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6875 - loss: 0.5480 - val_accuracy: 0.9167 - val_loss: 0.5513\n",
      "Epoch 99/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8931 - loss: 0.5282 - val_accuracy: 0.9500 - val_loss: 0.5420\n",
      "Epoch 100/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9147 - loss: 0.4939 - val_accuracy: 0.9667 - val_loss: 0.5340\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(pred)\n\u001b[1;32m      7\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2671\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m \n\u001b[1;32m   2565\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[38;5;124;03m<BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2668\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2670\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[0;32m-> 2671\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2674\u001b[0m     labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:107\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    104\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    109\u001b[0m             type_true, type_pred\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    114\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and binary targets"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(mms_X_train, y_train, epochs=100,\n",
    "                   batch_size=16,\n",
    "                   validation_data=(mms_X_test, y_test))\n",
    "pred = model.predict(mms_X_test)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred = pred[0].apply(lambda x:1 if x>=0.5 else 0)\n",
    "print(classification_report(y_test, pred))\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iris(Model):\n",
    "    def __init__(self):\n",
    "        super(Iris, self).__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692de58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb36d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ad526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c946b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4febcf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fdcc6df",
   "metadata": {},
   "source": [
    "SubcalssAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22142ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552fff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34642f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85e68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664b22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeca81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80556258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ae2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
